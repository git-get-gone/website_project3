[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Get to Know Jordan Vickers",
    "section": "",
    "text": "Hi, My name is Jordan Vickers. I am a soon to be graduate of St. Olaf College as a Physics and Math major with a concentration in Statistics and Data Science. I love any kind of problems that involve pattern recognition and enjoy keeping both my mind and body active! I hope to continuously update this site with new and exciting projects showcasing my aptitude and passion for working with data. On this website, I would like to highlight some of my projects I have contributed on throughout my time at St. Olaf."
  },
  {
    "objectID": "git-get-gone.github.io/about.html",
    "href": "git-get-gone.github.io/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "git-get-gone.github.io/index.html",
    "href": "git-get-gone.github.io/index.html",
    "title": "git-get-gone.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1 == 2\n\n[1] TRUE"
  },
  {
    "objectID": "mini2.html",
    "href": "mini2.html",
    "title": "Basketball Project",
    "section": "",
    "text": "For our project, we used data from the Sports Reference website. March Madness refers to the annual NCAA college basketball tournament which typically happens in March. With “March Madness” being a popular event that attracts the excitement of sports fans, and can potentially line the pockets of many gamblers, it serves as an interesting exercise to analyze the performance of the different teams who participate. We are compiling a dataset of both the conference performance and individual school performance. In order to do this, we used the rvest and polite package in order to scrape the contents of the site and build our dataset.\nBefore scraping however, we verified using the paths_allowed function to check the robots.txt which gave permission for scraping. We created functions in order to scrape the site over multiple years in order to gain a broader understanding of how performance has changed over time. Our analysis may provide insights into what teams may be standout picks based on previous performance and may highlight strengths and weaknesses that lie in a team alongside variables that affect their performance. We could also analyse what conferences have historically been the best and what that may mean with many conferences changing in recent years.\n\n# Step 1: Download the HTML and turn it into an XML file with read_html()\nbasketball_site &lt;- read_html(\"https://www.sports-reference.com/cbb/seasons/men/1993-advanced-school-stats.html\")\nbasketball_site\n\n{html_document}\n&lt;html data-version=\"klecko-\" data-root=\"/home/cbb/build\" lang=\"en\" class=\"no-js\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"cbb\"&gt;\\n&lt;div id=\"wrap\"&gt;\\n  \\n  &lt;div id=\"header\" role=\"banner\" ...\n\n\nAs you see above we first start off reading the pure html document. We will sift through these tags to find the table we want. We need to look at both the Advanced school stats and the conference data. The immediate code below functionizes the process so we can find the tables for varying years. The code below finds the table containing game data for each school.\n\n#Takes a list of years and produces a table for each\nbasketball_stats &lt;- function(year_list) { \n  \n basketball_stats_data_list &lt;- purrr::map(year_list, ~ basketball_scrape(year = .x) %&gt;% mutate(year = .x)) #Makes a column for each year\n \nbasketball_stats_all_years &lt;- list_rbind(basketball_stats_data_list)\n\nreturn(basketball_stats_all_years)\n\n}\n\n\n#Smaller function to grab data \nbasketball_scrape &lt;- function(year) {\n  \nsession &lt;- bow(str_c(\"https://www.sports-reference.com/cbb/seasons/\", year,\"-advanced-school-stats.html\"), force = TRUE)\n  \ntitle_temp &lt;- html_nodes(basketball_site, css = \"table\")\n\n#Table of interest is the first table\nBasketball_table &lt;- html_table(title_temp, header = T, fill = T)[[1]] %&gt;% row_to_names(row_number = 1) %&gt;% \n  clean_names() %&gt;%\n  select(-c(starts_with(\"na\"))) %&gt;%\n  select(1:16)\n\nreturn(Basketball_table)\n\n}\n\ntest&lt;- basketball_scrape(\"2000\")\ntest\n\n# A tibble: 326 × 16\n   rk    school      g     w     l     w_l_percent srs   sos   w_2   l_2   w_3  \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1     Air Force   28    9     19    .321        -7.45 2.05  3     15    7    \n 2 2     Akron       26    8     18    .308        -10.… -5.06 3     15    5    \n 3 3     Alabama     29    16    13    .552        9.66  7.83  7     9     12   \n 4 4     Alabama St… 27    14    13    .519        -8.49 -9.70 9     5     9    \n 5 5     Alcorn Sta… 27    7     20    .259        -11.… -4.58 5     9     3    \n 6 6     American    28    11    17    .393        -0.17 1.15  6     8     6    \n 7 7     Appalachia… 28    13    15    .464        -6.09 -4.27 8     10    8    \n 8 8     Arizona NC… 28    24    4     .857        20.50 8.28  17    1     14   \n 9 9     Arizona St… 28    18    10    .643        9.24  6.99  11    7     13   \n10 10    Arkansas N… 31    22    9     .710        17.72 8.30  10    6     12   \n# ℹ 316 more rows\n# ℹ 5 more variables: l_3 &lt;chr&gt;, w_4 &lt;chr&gt;, l_4 &lt;chr&gt;, tm &lt;chr&gt;, opp &lt;chr&gt;\n\nyears&lt;- c(\"2002\",\"2001\")\ntest2 &lt;- basketball_stats(years)\n\nThis code serves as an extension of this code in order to find conference data as well.\n\nbasketball_scrape_c &lt;- function(year) {\nurl &lt;- str_c(\"https://www.sports-reference.com/cbb/seasons/men/\", year, \".html\")\nrobotstxt::paths_allowed(url) # test to ensure it is fine to scrape\nnih &lt;- read_html(url)\ntitle_temp &lt;- html_nodes(nih, css = \"table\")\nBasketball_table &lt;- html_table(title_temp, header = TRUE, fill = TRUE)[[1]] # selecting the table we want\nBasketball_table &lt;- Basketball_table |&gt;\nmutate(year = year) # adding year as a column\nBasketball_table\n}\n\n\nconference_years &lt;- function(year_list) {\nconference_data_list &lt;- purrr::map(year_list, ~ basketball_scrape_c(year = .x) %&gt;% mutate(year = .x))\nconference_stats_all_years &lt;- list_rbind(conference_data_list)\nconference_stats_all_years\n}\n\nyears &lt;- c(\"2018\",\"2019\") # testing the code for multiple years\ntest4 &lt;- conference_years(years) # testing the new function\ntest4\n\n# A tibble: 64 × 14\n      Rk Conference       Schls     W     L `W-L%`   SRS   SOS    AP  NCAA    FF\n   &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1 Big East Confer…    10   213   128  0.625 14.2   9.28     0     6     1\n 2     2 Pac-12 Conferen…    12   231   172  0.573  8.61  6.15     1     3     0\n 3     3 Big 12 Conferen…    10   220   130  0.629 15.2   9.83     0     7     1\n 4     4 Southeastern Co…    14   286   193  0.597 13     9.34     0     8     0\n 5     5 Atlantic 10 Con…    14   232   224  0.509  2.26  1.99     0     3     0\n 6     6 Atlantic Coast …    15   315   200  0.612 13.4   8.51     0     9     0\n 7     7 Big Ten Confere…    14   289   189  0.605 13.0   7.99     0     4     1\n 8     8 West Coast Conf…    10   180   159  0.531  1.83  1.43     0     1     0\n 9     9 Conference USA      14   245   221  0.526  0.22  0.2      0     1     0\n10    10 American Athlet…    12   224   168  0.571  5.58  3.41     0     3     0\n# ℹ 54 more rows\n# ℹ 3 more variables: `Regular Season Champ` &lt;chr&gt;, `Tournament Champ` &lt;chr&gt;,\n#   year &lt;chr&gt;"
  },
  {
    "objectID": "mini1.html#interactive-plots",
    "href": "mini1.html#interactive-plots",
    "title": "Mapping Project",
    "section": "Interactive Plots",
    "text": "Interactive Plots\n\n\n\n\n\n\nObesity map\n\n#| warning: false\n#| message: false\n#| echo: false\n#| results: hide\n\n\n\n####################\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n###################\n\n\nobesity_pal &lt;- colorFactor(viridis(2), states$above_below) #This creates a color factor for above and below\n\nWarning: Unknown or uninitialised column: `above_below`.\n\nobesity &lt;- obesity |&gt;\n  mutate(labels = str_c(str_to_title(state), \": \", Obesity)) %&gt;% #show the obesity rate when you hover over\n  arrange(state) #need to arrange so when merged with states data it lines up correctly for obesity\n\nlabels &lt;- lapply(obesity$labels, HTML) #\n\nobesity &lt;- obesity %&gt;%\n   filter(state != \"district of columbia\" & state != \"puerto rico\" ) #do the same for obesity just in case\n\nstates &lt;- states %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%\n  filter(name != \"district of columbia\" & name != \"puerto rico\" ) #need to remove these two from data since obesity data does not have district of columbia and puerto rico \n\nstates$above_below &lt;- obesity$above_below # Add above below to states data\n\n# obesity %&gt;%\n#   anti_join(states, by = c(\"state\" = \"name\"))\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~obesity_pal(above_below), #use the pal from above\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = obesity_pal, values = obesity$above_below, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data"
  },
  {
    "objectID": "mini1.html",
    "href": "mini1.html",
    "title": "Mapping Project",
    "section": "",
    "text": "Static Plots\nThe first statistic I would like to investigate are the literacy rates among states. Literacy rates are incredibly important and have sweeping effects on people’s employment opportunities, educational attainment and overall quality of life. The data comes from the The World Population Review though it does not state the exact literacy rates. In actuality, it gives the percentage of the adult population that fall under the category of low literacy rates for 2024. According to the National Center for Education Statistics, low literacy is defined as “those performing at PIAAC literacy proficiency level 1 or below or who could not participate due to a language barrier or a cognitive or physical inability to be interviewed.” These adults are able to process meaning at the sentence level meaning they can only read short, simple paragraphs. (https://nces.ed.gov/surveys/piaac/measure.asp)\nIn order to accomplish this graph, we use geometic map data of the US states.\n\nus_states &lt;- map_data(\"state\") #This contains the geometric map data\nliteracy &lt;- read_csv(\"~/Desktop/data science/us.-literacy-rates-by-state-2024 (1).csv\") \n\n\nliteracy &lt;- literacy |&gt;\n  mutate(state = str_to_lower(state)) %&gt;%\n  rename(low_lit = LiteracyRatesPercofPopulationWithLowLiteracy)\n\nliteracy |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt; #state in literacy and region in us_states\n  rename(region = state) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = low_lit), color = \"black\")+\n  labs(\n    title = \"Map of the US showing % of State Population with Low Literacy\",\n    fill = \"% of Population with Low Literacy\",\n    caption = \"Source: The World Population Review\"\n  )+\n  coord_map() + #makes it scale correctly \n  theme_void() + \n  scale_fill_viridis(option = \"E\")\n\n\n\n\n\n\n\n\nThis is a color coded choropleth map showing the percentage of the state population that has a low literacy rate. The legend shows that the percentage of low literacy ranges from about 0 to over 25% with darker blue colors representing a small percentage of low literacy ranging to brighter yellow that represents a higher percentage of low literacy. We notice that southern states appear to have higher percentages of low literacy as notice they appear more yellow in color. Texas, California and New Mexico stand out as having the highest percentage. In contrast, northern states appear to have lower percentages of low literacy especially upper New England such as New Hampshire and Midwest states like Minnesota and Montana.\n\nMy second statistic I would like to to investigate are obesity rates in America. The data was sourced from Data.Gov gives the state obesity rate for 2015. Obesity is a growing problem in America, as it poses significant health risks to thousands of people. However different states have varying levels of obesity. Here we can investigate which states are above and below the average obesity percentage in America to see if there are any patterns in obesity rates.\n\nobesity &lt;- read_csv(\"~/Desktop/data science/LakeCounty_Health_-6177935595181947989.csv\")\n\nobesity &lt;- obesity %&gt;%\n  rename(state = NAME) %&gt;%\n  mutate(state = str_to_lower(state)) %&gt;%\n  mutate(above_below = ifelse(Obesity &gt; mean(Obesity), \"Above\", \"Below\")) \n\n\nobesity |&gt;\n  mutate(state = str_to_lower(state)) %&gt;%\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  rename(region = state) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = above_below), color = \"darkgrey\", linewidth = 0.4) + \n  labs(fill = \"Above or Below Mean\",\ncaption = \"Source: Data.Gov\",\ntitle = \"Graph showing states with Obesity Rates Above or Below Average Rate\") +\n  coord_map() + \n  theme_void() +\n  scale_fill_viridis(discrete = TRUE, option = \"E\") #discrete = T makes it work for categorical data\n\n\n\n\n\n\n\n  #scale_fill_manual()\n\nFrom the graph above, we notice that the majority of states with obesity rates above the average are in the middle of the United States on the right side. The Left side of the United States remains mostly below the average apart from Oregon. We also notice Minnesota as anomaly in the middle being below the average obesity rate. This may point to cultural reasons for obesity, as many adjacent states share the same category, likely having similar food customs or ways of life that may lead to obesity. The spread is also vertical with similar categories spanning the length of the US and being split in half by the breadth. This map may be further enhanced if it could be cross referenced with a map showing the prevalence of fast food chains such as McDonald’s.\n\n\n\nInteractive Plots\nBelow are more plot that provide interactive information on literacy. Here you can hover over a state and see the obesity and literary rate so that you can easily compare it with their neighboring states. Both plots make use of sf data in conjunction with the leaflet package. Here I transform the obesity and literacy data to match the provided geometric data pulled from a github repository.\n\nlibrary(sf) \n\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")  \n\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0,11,15,20,25,29,Inf)\npal &lt;- colorBin(\"inferno\", domain = literacy$low_lit, bins = bins)\n\nliteracy &lt;- literacy |&gt;\n  mutate(labels = str_c(str_to_title(state), \": \", low_lit))\n\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n\nlabels &lt;- lapply(literacy$labels, HTML)\n\nstates &lt;- states %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%\n  filter(name != \"district of columbia\" & name != \"puerto rico\" )\n\n\nstates$low_lit &lt;- literacy$low_lit #need to add it like this so that the spacial data remains because joining causes it to lose the sf\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(low_lit),\n    weight = 2,\n    opacity = 1,\n    smoothFactor = 0.5,\n    color = \"white\",\n    dashArray = \"2\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~literacy$low_lit, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\nObesity map\n\nobesity_pal &lt;- colorFactor(viridis(2), states$above_below) #This creates a color factor for above and below\n\nobesity &lt;- obesity |&gt;\n  mutate(labels = str_c(str_to_title(state), \": \", Obesity)) %&gt;% #show the obesity rate when you hover over\n  arrange(state) #need to arrange so when merged with states data it lines up correctly for obesity\n\nlabels &lt;- lapply(obesity$labels, HTML) #\n\nobesity &lt;- obesity %&gt;%\n   filter(state != \"district of columbia\" & state != \"puerto rico\" ) #do the same for obesity just in case\n\nstates &lt;- states %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%\n  filter(name != \"district of columbia\" & name != \"puerto rico\" ) #need to remove these two from data since obesity data does not have district of columbia and puerto rico \n\nstates$above_below &lt;- obesity$above_below # Add above below to states data\n\n# obesity %&gt;%\n#   anti_join(states, by = c(\"state\" = \"name\"))\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~obesity_pal(above_below), #use the pal from above\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = obesity_pal, values = obesity$above_below, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\nLiteracy Map"
  },
  {
    "objectID": "miniporject4.html",
    "href": "miniporject4.html",
    "title": "Youtube Video Analysis",
    "section": "",
    "text": "Youtube first started as a simple domain in 2005 that allowed anyone to upload and share videos as they pleased. It has now grown into a multibillion enterprise that has become the defacto video sharing platform on the internet. Currently, many people make their livelihood on Youtube and as such use, people are constantly finding ways to boost their visibility on the platform and build their channel.\nIn this project, I seek to use a kaggle database of Youtube Statistics to analyse aspects of a video to see what increases metrics such as its views and likes. Additionally, I seek to assess how comments and their perceived sentiment affect a video’s performance.\n\n\n\ncomments &lt;- read_csv(\"~/Project 3/Youtube Statistics/comments.csv\")\n\nNew names:\nRows: 18409 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): Video ID, Comment dbl (3): ...1, Likes, Sentiment\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nstats &lt;- read_csv(\"~/Project 3/Youtube Statistics/videos-stats.csv\")\n\nNew names:\nRows: 1881 Columns: 8\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): Title, Video ID, Keyword dbl (4): ...1, Likes, Comments, Views date (1):\nPublished At\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n#---------------------------------------\n\ncomments &lt;- comments %&gt;%\n            rename(vid_id = `Video ID`) %&gt;%\n            select(-Sentiment)\n\nstats &lt;- stats %&gt;%\n        rename(vid_id =`Video ID`) %&gt;%\n        rename(published = `Published At`)\n\ncomments &lt;- comments %&gt;% \n   filter(!(str_detect(Comment, \"[^\\x01-\\x7F]+\")))  #Filter only english comments with english characters by using a regular expression that gets rid of items not in unicode. \n#(Regex 1) (Str_ Function 1)\n\n\ncomments_vids &lt;- comments %&gt;%\n  left_join(stats, by = \"vid_id\")\n\nWarning in left_join(., stats, by = \"vid_id\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 17 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n#-------------------------------------------------\n#Split the comments into the individual words\nwords_comments &lt;- comments_vids %&gt;%\nunnest_tokens(word, Comment, token = \"words\") %&gt;%\n  rename(Comment_likes = Likes.x, Video_likes = Likes.y )\n\n#Split the title into the individual words\nwords_title &lt;- comments_vids %&gt;%\nunnest_tokens(word, Title, token = \"words\") %&gt;%\n  rename(Comment_likes = Likes.x, Video_likes = Likes.y )\n\n#----------------------------------------------------\n\n#Sentiments\n\nbing_sentiments &lt;- get_sentiments(lexicon = \"bing\")\nafinn_sentiments &lt;- get_sentiments(lexicon = \"afinn\")\nnrc_sentiments &lt;- get_sentiments(lexicon = \"nrc\")\n\n\n\n\n\n#Different keywords that I will use as categories\nstats %&gt;%\n  group_by(Keyword) %&gt;%\n  summarise(mean_views = mean(Views), mean_comments = mean(Comments), mean_likes = mean(Likes) ) %&gt;%\n  arrange(desc(mean_likes)) \n\n# A tibble: 41 × 4\n   Keyword  mean_views mean_comments mean_likes\n   &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 mrbeast   66764004.        95944.   2105914.\n 2 animals   94723961.        21872.    760776.\n 3 bed       53893229.        15181.    473682.\n 4 google   103364971.        22646.    471167.\n 5 music     29364893.        12289.    314188.\n 6 cubes     15038739.         6833.    303061.\n 7 history   15047130.        20190.    273411.\n 8 marvel     6614080.         9701.    210379.\n 9 tutorial   6761032.         7935.    167721.\n10 how-to     7809285.         6951.    158898.\n# ℹ 31 more rows\n\nstats %&gt;%\n  ggplot(aes(x =Views ))+\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\nstats %&gt;%\n  ggplot(aes(x = Likes ))+\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\nmosaic::favstats( ~Views, data = stats)\n\n min    Q1 median      Q3        max     mean        sd    n missing\n  25 84515 591721 2804978 4034122271 11612916 108444994 1879       2\n\nstats %&gt;%\n  filter(Views &lt; 10000000) %&gt;%\n  ggplot(aes(x = Views ))+\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\n\n\n\n\nAnother key thing to look at is how these statistics vary among different keywords. For the sake of this project I will use these keywords as categories. Though some keywords do include very specific things such as “Mrbeast” or “Apple”, so I will choose a few broad topics to analyse. In order to assess a wide range, I have chosen “Animals, Tech, Education, Sports” and I added “data science” for my own curiosity.\n\ncategories &lt;- c(\"animals\", \"tech\", \"education\", \"sports\", \"data science\")\n\ncomments_vids_reduced &lt;-comments_vids %&gt;%\n  filter(Keyword %in% categories )\n\nJust for fun we can generate word clouds to see what people are talking about and what the titles for data science videos look like.\n\nwords &lt;- words_title |&gt;\n  filter(Keyword == \"data science\" ) |&gt;\n  anti_join(stop_words) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\n\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 500, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(6, \"Dark2\"))\n\n\n\n\n\n\n\n\n\nwords &lt;- words_comments |&gt;\n  filter(Keyword == \"data science\" ) |&gt;\n  anti_join(stop_words) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\n\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 500, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(9,\"PuOr\")\n)\n\n\n\n\n\n\n\n\n\n\n\nComments are an indispensable part of the Youtube experience as it allows people to give feedback, tell jokes and ultimately build a community around a channel. In this section, I will do some analysis of these comments to see if we can find any trends in what people are commenting and the sentiment.\nOne common thing that comments do is quote key lines from the video. These comments remind people of funny or stand out moments in a video usually reinforcing the impact. I would to investigate if these comments tend to get more engagement from others.\n\nquoted &lt;- comments %&gt;%\n  mutate(quote = str_extract_all(str_to_lower(Comment), '\".*\\\"')) %&gt;%\n  mutate(quote_bin = str_detect(str_to_lower(Comment), '\".*\\\"'))\n\nquoted %&gt;% \n  filter(Likes &lt; 500) %&gt;% #Mean likes is 1000, median is about 35, 500 seems to be a reasonable amount since most comments don't pass this mark \n  ggplot(aes(x = Likes, y = quote_bin, fill = quote_bin ))+\n  geom_density_ridges(alpha = 0.5) +\n  labs (\n    y = \"Does the comment quote?\",\n    x = \"Number of Likes on the Comment\",\n    fill = \"Quote in Comment?\"\n  )\n\n\n\n\n\n\n\n\nThe graph above is a density plot for comments that include a quote. We see that the number of likes on a comment that includes a quote and doesn’t have very similar distributions. The comments that do include a quote however appear to have a slightly higher mean value of likes and has a higher proportion of highly liked comments. It appears that my hypothesis may be correct.\n\n\nAnother interesting aspects of comments is how varied the sentiment can be.\n\nwords_comments |&gt;\n  inner_join(bing_sentiments) |&gt;\n  filter(Keyword %in% categories ) |&gt;\n  count(sentiment, word, Keyword, sort = TRUE) |&gt;\n  group_by(sentiment, Keyword) |&gt;\n  slice_max(n, n = 3) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +\n    geom_col() +  \n    coord_flip() +\n    facet_grid(Keyword ~ sentiment, scales = \"free\") + \n  labs(\n    x= \"Top 3 Words\",\n    y = \"Number of occurences\"\n  )\n\n\n\n\n\n\n\n\nThis plot above is a coded bar graph of negative and positive sentiment of words in the different categories. For the sake of reducing clutter, I have limited the choice to the top three contributing words for each category, both negative and positive. We see the words “like”, “love” and “great” are common in almost all categories for contributing to the positve sentiment of comments. There is however more diversity in the negative words with unique ones like “Shark” being a negative for animals and words like “problems” and “hard” being negative in data science and education. For each category, the negative words seem congruent with the categories while positve words seem more generic.\n\n\n\n\n\n\nIn order to stand out and grab attention, many videos have opted to have flashy titles that grab attention. A simple way to do this is to just add more capital letters since they convey a sense of urgency and may lend to more people clicking. For a more comprehensive view of the impact of capital letters, I have included all categories.\n\ncomments_vids_capitals &lt;- comments_vids %&gt;% \n  mutate(capitals = str_count(Title, \"\\\\b[A-Z]{2,}\\\\b\")) %&gt;% #Regex 2 #String function #2\n  rename(Comment_likes = Likes.x, Video_likes = Likes.y )\n\n\ncomments_vids_capitals %&gt;%\n  group_by(vid_id) %&gt;%\n  summarise(mean_views = median(Views), mean_comments = mean(Comments), mean_likes = mean(Video_likes), mean_cap = mean(capitals)) %&gt;%  #Using mean here to just get the number since I grouped by video. \n  filter(mean_views &lt; 3000000) %&gt;% \n  ggplot(aes(x = mean_cap, y = mean_views))+\n  geom_jitter()+\n  geom_smooth(se = T) +\n  labs(\n    x = \"Number of All Capital Words\",\n    y = \"Views\"\n  )\n\n\n\n\n\n\n\n\nThe above graph shows the number of views vs the number of all capital words in the title. We see the x-axis ranges form 0 to 16 and the number of views ranges from 0 to 3 million (videos above this value act as outliers). We see the majority of videos don’t contain less than 5 all capital words and we see a lot of variation in views for these videos. Contrary to my hypothesis, we see that actually videos with more all captial letters appear to have even less views than those who have less. This effect may be affected by the popularity of the channel itself. Additionally, flashy titles may have the oopposite effect since people may assume they are clickbait.\n\n\n\n\nAnother aspect I would like to investigate is the sentiment of the titles. I am of the belief that social media tends to be more polarizing and videos try to exacerbate negative stories in order to bring more attention. We can test this correlation and see if titles are trending towards negative sentiment.\n\nwords_title %&gt;%\n  inner_join(afinn_sentiments) %&gt;%\n  group_by(vid_id) %&gt;%\n  summarise(mean_views = median(Views), mean_comments = mean(Comments), mean_likes = mean(Video_likes), mean_sentiment = mean(value)) %&gt;%\n  filter(mean_views &lt;  3000000) %&gt;%\n  ggplot(aes(x = mean_sentiment, y = mean_views))+\n  geom_jitter()+\n  geom_smooth( method = \"lm\", se = T) +\n  labs (\n    x = \"Sentiment of the title\",\n    y = \"Views\"\n  )\n\n\n\n\n\n\n\n\nThe graph above shows the sentiment of a video title compared to the ammount of views the video had. We see the sentiment value ranges from -4 to 4 and the number of views ranges from 0 to 3 million. We see that there does not seem to be a strong relationship between the sentiment of the title and the number of views on the video. There is a large amount of variation in the views for almost all the sentiments, so we see that there may be no significant reason to make a title more negative or more positive."
  },
  {
    "objectID": "miniporject4.html#setting-up-the-data",
    "href": "miniporject4.html#setting-up-the-data",
    "title": "Youtube Video Analysis",
    "section": "",
    "text": "comments &lt;- read_csv(\"~/Project 3/Youtube Statistics/comments.csv\")\n\nNew names:\nRows: 18409 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): Video ID, Comment dbl (3): ...1, Likes, Sentiment\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nstats &lt;- read_csv(\"~/Project 3/Youtube Statistics/videos-stats.csv\")\n\nNew names:\nRows: 1881 Columns: 8\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): Title, Video ID, Keyword dbl (4): ...1, Likes, Comments, Views date (1):\nPublished At\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n#---------------------------------------\n\ncomments &lt;- comments %&gt;%\n            rename(vid_id = `Video ID`) %&gt;%\n            select(-Sentiment)\n\nstats &lt;- stats %&gt;%\n        rename(vid_id =`Video ID`) %&gt;%\n        rename(published = `Published At`)\n\ncomments &lt;- comments %&gt;% \n   filter(!(str_detect(Comment, \"[^\\x01-\\x7F]+\")))  #Filter only english comments with english characters by using a regular expression that gets rid of items not in unicode. \n#(Regex 1) (Str_ Function 1)\n\n\ncomments_vids &lt;- comments %&gt;%\n  left_join(stats, by = \"vid_id\")\n\nWarning in left_join(., stats, by = \"vid_id\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 17 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n#-------------------------------------------------\n#Split the comments into the individual words\nwords_comments &lt;- comments_vids %&gt;%\nunnest_tokens(word, Comment, token = \"words\") %&gt;%\n  rename(Comment_likes = Likes.x, Video_likes = Likes.y )\n\n#Split the title into the individual words\nwords_title &lt;- comments_vids %&gt;%\nunnest_tokens(word, Title, token = \"words\") %&gt;%\n  rename(Comment_likes = Likes.x, Video_likes = Likes.y )\n\n#----------------------------------------------------\n\n#Sentiments\n\nbing_sentiments &lt;- get_sentiments(lexicon = \"bing\")\nafinn_sentiments &lt;- get_sentiments(lexicon = \"afinn\")\nnrc_sentiments &lt;- get_sentiments(lexicon = \"nrc\")"
  },
  {
    "objectID": "miniporject4.html#eda",
    "href": "miniporject4.html#eda",
    "title": "Youtube Video Analysis",
    "section": "",
    "text": "#Different keywords that I will use as categories\nstats %&gt;%\n  group_by(Keyword) %&gt;%\n  summarise(mean_views = mean(Views), mean_comments = mean(Comments), mean_likes = mean(Likes) ) %&gt;%\n  arrange(desc(mean_likes)) \n\n# A tibble: 41 × 4\n   Keyword  mean_views mean_comments mean_likes\n   &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 mrbeast   66764004.        95944.   2105914.\n 2 animals   94723961.        21872.    760776.\n 3 bed       53893229.        15181.    473682.\n 4 google   103364971.        22646.    471167.\n 5 music     29364893.        12289.    314188.\n 6 cubes     15038739.         6833.    303061.\n 7 history   15047130.        20190.    273411.\n 8 marvel     6614080.         9701.    210379.\n 9 tutorial   6761032.         7935.    167721.\n10 how-to     7809285.         6951.    158898.\n# ℹ 31 more rows\n\nstats %&gt;%\n  ggplot(aes(x =Views ))+\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\nstats %&gt;%\n  ggplot(aes(x = Likes ))+\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\nmosaic::favstats( ~Views, data = stats)\n\n min    Q1 median      Q3        max     mean        sd    n missing\n  25 84515 591721 2804978 4034122271 11612916 108444994 1879       2\n\nstats %&gt;%\n  filter(Views &lt; 10000000) %&gt;%\n  ggplot(aes(x = Views ))+\n  geom_histogram(bins = 30)"
  },
  {
    "objectID": "miniporject4.html#keywords",
    "href": "miniporject4.html#keywords",
    "title": "Youtube Video Analysis",
    "section": "",
    "text": "Another key thing to look at is how these statistics vary among different keywords. For the sake of this project I will use these keywords as categories. Though some keywords do include very specific things such as “Mrbeast” or “Apple”, so I will choose a few broad topics to analyse. In order to assess a wide range, I have chosen “Animals, Tech, Education, Sports” and I added “data science” for my own curiosity.\n\ncategories &lt;- c(\"animals\", \"tech\", \"education\", \"sports\", \"data science\")\n\ncomments_vids_reduced &lt;-comments_vids %&gt;%\n  filter(Keyword %in% categories )\n\nJust for fun we can generate word clouds to see what people are talking about and what the titles for data science videos look like.\n\nwords &lt;- words_title |&gt;\n  filter(Keyword == \"data science\" ) |&gt;\n  anti_join(stop_words) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\n\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 500, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(6, \"Dark2\"))\n\n\n\n\n\n\n\n\n\nwords &lt;- words_comments |&gt;\n  filter(Keyword == \"data science\" ) |&gt;\n  anti_join(stop_words) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\n\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 500, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(9,\"PuOr\")\n)"
  },
  {
    "objectID": "miniporject4.html#comments",
    "href": "miniporject4.html#comments",
    "title": "Youtube Video Analysis",
    "section": "",
    "text": "Comments are an indispensable part of the Youtube experience as it allows people to give feedback, tell jokes and ultimately build a community around a channel. In this section, I will do some analysis of these comments to see if we can find any trends in what people are commenting and the sentiment.\nOne common thing that comments do is quote key lines from the video. These comments remind people of funny or stand out moments in a video usually reinforcing the impact. I would to investigate if these comments tend to get more engagement from others.\n\nquoted &lt;- comments %&gt;%\n  mutate(quote = str_extract_all(str_to_lower(Comment), '\".*\\\"')) %&gt;%\n  mutate(quote_bin = str_detect(str_to_lower(Comment), '\".*\\\"'))\n\nquoted %&gt;% \n  filter(Likes &lt; 500) %&gt;% #Mean likes is 1000, median is about 35, 500 seems to be a reasonable amount since most comments don't pass this mark \n  ggplot(aes(x = Likes, y = quote_bin, fill = quote_bin ))+\n  geom_density_ridges(alpha = 0.5) +\n  labs (\n    y = \"Does the comment quote?\",\n    x = \"Number of Likes on the Comment\",\n    fill = \"Quote in Comment?\"\n  )\n\n\n\n\n\n\n\n\nThe graph above is a density plot for comments that include a quote. We see that the number of likes on a comment that includes a quote and doesn’t have very similar distributions. The comments that do include a quote however appear to have a slightly higher mean value of likes and has a higher proportion of highly liked comments. It appears that my hypothesis may be correct.\n\n\nAnother interesting aspects of comments is how varied the sentiment can be.\n\nwords_comments |&gt;\n  inner_join(bing_sentiments) |&gt;\n  filter(Keyword %in% categories ) |&gt;\n  count(sentiment, word, Keyword, sort = TRUE) |&gt;\n  group_by(sentiment, Keyword) |&gt;\n  slice_max(n, n = 3) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +\n    geom_col() +  \n    coord_flip() +\n    facet_grid(Keyword ~ sentiment, scales = \"free\") + \n  labs(\n    x= \"Top 3 Words\",\n    y = \"Number of occurences\"\n  )\n\n\n\n\n\n\n\n\nThis plot above is a coded bar graph of negative and positive sentiment of words in the different categories. For the sake of reducing clutter, I have limited the choice to the top three contributing words for each category, both negative and positive. We see the words “like”, “love” and “great” are common in almost all categories for contributing to the positve sentiment of comments. There is however more diversity in the negative words with unique ones like “Shark” being a negative for animals and words like “problems” and “hard” being negative in data science and education. For each category, the negative words seem congruent with the categories while positve words seem more generic."
  },
  {
    "objectID": "miniporject4.html#titles",
    "href": "miniporject4.html#titles",
    "title": "Youtube Video Analysis",
    "section": "",
    "text": "In order to stand out and grab attention, many videos have opted to have flashy titles that grab attention. A simple way to do this is to just add more capital letters since they convey a sense of urgency and may lend to more people clicking. For a more comprehensive view of the impact of capital letters, I have included all categories.\n\ncomments_vids_capitals &lt;- comments_vids %&gt;% \n  mutate(capitals = str_count(Title, \"\\\\b[A-Z]{2,}\\\\b\")) %&gt;% #Regex 2 #String function #2\n  rename(Comment_likes = Likes.x, Video_likes = Likes.y )\n\n\ncomments_vids_capitals %&gt;%\n  group_by(vid_id) %&gt;%\n  summarise(mean_views = median(Views), mean_comments = mean(Comments), mean_likes = mean(Video_likes), mean_cap = mean(capitals)) %&gt;%  #Using mean here to just get the number since I grouped by video. \n  filter(mean_views &lt; 3000000) %&gt;% \n  ggplot(aes(x = mean_cap, y = mean_views))+\n  geom_jitter()+\n  geom_smooth(se = T) +\n  labs(\n    x = \"Number of All Capital Words\",\n    y = \"Views\"\n  )\n\n\n\n\n\n\n\n\nThe above graph shows the number of views vs the number of all capital words in the title. We see the x-axis ranges form 0 to 16 and the number of views ranges from 0 to 3 million (videos above this value act as outliers). We see the majority of videos don’t contain less than 5 all capital words and we see a lot of variation in views for these videos. Contrary to my hypothesis, we see that actually videos with more all captial letters appear to have even less views than those who have less. This effect may be affected by the popularity of the channel itself. Additionally, flashy titles may have the oopposite effect since people may assume they are clickbait."
  },
  {
    "objectID": "miniporject4.html#sentiment-of-the-titles",
    "href": "miniporject4.html#sentiment-of-the-titles",
    "title": "Youtube Video Analysis",
    "section": "",
    "text": "Another aspect I would like to investigate is the sentiment of the titles. I am of the belief that social media tends to be more polarizing and videos try to exacerbate negative stories in order to bring more attention. We can test this correlation and see if titles are trending towards negative sentiment.\n\nwords_title %&gt;%\n  inner_join(afinn_sentiments) %&gt;%\n  group_by(vid_id) %&gt;%\n  summarise(mean_views = median(Views), mean_comments = mean(Comments), mean_likes = mean(Video_likes), mean_sentiment = mean(value)) %&gt;%\n  filter(mean_views &lt;  3000000) %&gt;%\n  ggplot(aes(x = mean_sentiment, y = mean_views))+\n  geom_jitter()+\n  geom_smooth( method = \"lm\", se = T) +\n  labs (\n    x = \"Sentiment of the title\",\n    y = \"Views\"\n  )\n\n\n\n\n\n\n\n\nThe graph above shows the sentiment of a video title compared to the ammount of views the video had. We see the sentiment value ranges from -4 to 4 and the number of views ranges from 0 to 3 million. We see that there does not seem to be a strong relationship between the sentiment of the title and the number of views on the video. There is a large amount of variation in the views for almost all the sentiments, so we see that there may be no significant reason to make a title more negative or more positive."
  }
]