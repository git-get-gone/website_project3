[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Get to Know Jordan Vickers",
    "section": "",
    "text": "Hi, My name is Jordan Vickers. I am a soon to be graduate of St. Olaf College as a Physics and Math major with a concentration in Statistics and Data Science. I love any kind of problems that involve pattern recognition and enjoy keeping both my mind and body active! I hope to continuously update this site with new and exciting projects showcasing my aptitude and passion for working with data. On this website, I would like to highlight some of my projects I have contributed on throughout my time at St. Olaf."
  },
  {
    "objectID": "git-get-gone.github.io/about.html",
    "href": "git-get-gone.github.io/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "git-get-gone.github.io/index.html",
    "href": "git-get-gone.github.io/index.html",
    "title": "git-get-gone.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1 == 2\n\n[1] TRUE"
  },
  {
    "objectID": "mini2.html",
    "href": "mini2.html",
    "title": "Basketball Project",
    "section": "",
    "text": "For our project, we used data from the Sports Reference website. March Madness refers to the annual NCAA college basketball tournament which typically happens in March. With “March Madness” being a popular event that attracts the excitement of sports fans, and can potentially line the pockets of many gamblers, it serves as an interesting exercise to analyze the performance of the different teams who participate. We are compiling a dataset of both the conference performance and individual school performance. In order to do this, we used the rvest and polite package in order to scrape the contents of the site and build our dataset.\nBefore scraping however, we verified using the paths_allowed function to check the robots.txt which gave permission for scraping. We created functions in order to scrape the site over multiple years in order to gain a broader understanding of how performance has changed over time. Our analysis may provide insights into what teams may be standout picks based on previous performance and may highlight strengths and weaknesses that lie in a team alongside variables that affect their performance. We could also analyse what conferences have historically been the best and what that may mean with many conferences changing in recent years.\n\n# Step 1: Download the HTML and turn it into an XML file with read_html()\nbasketball_site &lt;- read_html(\"https://www.sports-reference.com/cbb/seasons/men/1993-advanced-school-stats.html\")\nbasketball_site\n\n{html_document}\n&lt;html data-version=\"klecko-\" data-root=\"/home/cbb/build\" lang=\"en\" class=\"no-js\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"cbb\"&gt;\\n&lt;div id=\"wrap\"&gt;\\n  \\n  &lt;div id=\"header\" role=\"banner\" ...\n\n\nAs you see above we first start off reading the pure html document. We will sift through these tags to find the table we want. We need to look at both the Advanced school stats and the conference data. The immediate code below functionizes the process so we can find the tables for varying years. The code below finds the table containing game data for each school.\n\n#Takes a list of years and produces a table for each\nbasketball_stats &lt;- function(year_list) { \n  \n basketball_stats_data_list &lt;- purrr::map(year_list, ~ basketball_scrape(year = .x) %&gt;% mutate(year = .x)) #Makes a column for each year\n \nbasketball_stats_all_years &lt;- list_rbind(basketball_stats_data_list)\n\nreturn(basketball_stats_all_years)\n\n}\n\n\n#Smaller function to grab data \nbasketball_scrape &lt;- function(year) {\n  \nsession &lt;- bow(str_c(\"https://www.sports-reference.com/cbb/seasons/\", year,\"-advanced-school-stats.html\"), force = TRUE)\n  \ntitle_temp &lt;- html_nodes(basketball_site, css = \"table\")\n\n#Table of interest is the first table\nBasketball_table &lt;- html_table(title_temp, header = T, fill = T)[[1]] %&gt;% row_to_names(row_number = 1) %&gt;% \n  clean_names() %&gt;%\n  select(-c(starts_with(\"na\"))) %&gt;%\n  select(1:16)\n\nreturn(Basketball_table)\n\n}\n\ntest&lt;- basketball_scrape(\"2000\")\ntest\n\n# A tibble: 326 × 16\n   rk    school      g     w     l     w_l_percent srs   sos   w_2   l_2   w_3  \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1     Air Force   28    9     19    .321        -7.45 2.05  3     15    7    \n 2 2     Akron       26    8     18    .308        -10.… -5.06 3     15    5    \n 3 3     Alabama     29    16    13    .552        9.66  7.83  7     9     12   \n 4 4     Alabama St… 27    14    13    .519        -8.49 -9.70 9     5     9    \n 5 5     Alcorn Sta… 27    7     20    .259        -11.… -4.58 5     9     3    \n 6 6     American    28    11    17    .393        -0.17 1.15  6     8     6    \n 7 7     Appalachia… 28    13    15    .464        -6.09 -4.27 8     10    8    \n 8 8     Arizona NC… 28    24    4     .857        20.50 8.28  17    1     14   \n 9 9     Arizona St… 28    18    10    .643        9.24  6.99  11    7     13   \n10 10    Arkansas N… 31    22    9     .710        17.72 8.30  10    6     12   \n# ℹ 316 more rows\n# ℹ 5 more variables: l_3 &lt;chr&gt;, w_4 &lt;chr&gt;, l_4 &lt;chr&gt;, tm &lt;chr&gt;, opp &lt;chr&gt;\n\nyears&lt;- c(\"2002\",\"2001\")\ntest2 &lt;- basketball_stats(years)\n\nThis code serves as an extension of this code in order to find conference data as well.\n\nbasketball_scrape_c &lt;- function(year) {\nurl &lt;- str_c(\"https://www.sports-reference.com/cbb/seasons/men/\", year, \".html\")\nrobotstxt::paths_allowed(url) # test to ensure it is fine to scrape\nnih &lt;- read_html(url)\ntitle_temp &lt;- html_nodes(nih, css = \"table\")\nBasketball_table &lt;- html_table(title_temp, header = TRUE, fill = TRUE)[[1]] # selecting the table we want\nBasketball_table &lt;- Basketball_table |&gt;\nmutate(year = year) # adding year as a column\nBasketball_table\n}\n\n\nconference_years &lt;- function(year_list) {\nconference_data_list &lt;- purrr::map(year_list, ~ basketball_scrape_c(year = .x) %&gt;% mutate(year = .x))\nconference_stats_all_years &lt;- list_rbind(conference_data_list)\nconference_stats_all_years\n}\n\nyears &lt;- c(\"2018\",\"2019\") # testing the code for multiple years\ntest4 &lt;- conference_years(years) # testing the new function\ntest4\n\n# A tibble: 64 × 14\n      Rk Conference       Schls     W     L `W-L%`   SRS   SOS    AP  NCAA    FF\n   &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1 Big East Confer…    10   213   128  0.625 14.2   9.28     0     6     1\n 2     2 Pac-12 Conferen…    12   231   172  0.573  8.61  6.15     1     3     0\n 3     3 Big 12 Conferen…    10   220   130  0.629 15.2   9.83     0     7     1\n 4     4 Southeastern Co…    14   286   193  0.597 13     9.34     0     8     0\n 5     5 Atlantic 10 Con…    14   232   224  0.509  2.26  1.99     0     3     0\n 6     6 Atlantic Coast …    15   315   200  0.612 13.4   8.51     0     9     0\n 7     7 Big Ten Confere…    14   289   189  0.605 13.0   7.99     0     4     1\n 8     8 West Coast Conf…    10   180   159  0.531  1.83  1.43     0     1     0\n 9     9 Conference USA      14   245   221  0.526  0.22  0.2      0     1     0\n10    10 American Athlet…    12   224   168  0.571  5.58  3.41     0     3     0\n# ℹ 54 more rows\n# ℹ 3 more variables: `Regular Season Champ` &lt;chr&gt;, `Tournament Champ` &lt;chr&gt;,\n#   year &lt;chr&gt;"
  },
  {
    "objectID": "mini1.html#interactive-plots",
    "href": "mini1.html#interactive-plots",
    "title": "Mapping Project",
    "section": "Interactive Plots",
    "text": "Interactive Plots\n\n\n\n\n\n\nObesity map\n\n#| warning: false\n#| message: false\n#| echo: false\n#| results: hide\n\n\n\n####################\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n###################\n\n\nobesity_pal &lt;- colorFactor(viridis(2), states$above_below) #This creates a color factor for above and below\n\nWarning: Unknown or uninitialised column: `above_below`.\n\nobesity &lt;- obesity |&gt;\n  mutate(labels = str_c(str_to_title(state), \": \", Obesity)) %&gt;% #show the obesity rate when you hover over\n  arrange(state) #need to arrange so when merged with states data it lines up correctly for obesity\n\nlabels &lt;- lapply(obesity$labels, HTML) #\n\nobesity &lt;- obesity %&gt;%\n   filter(state != \"district of columbia\" & state != \"puerto rico\" ) #do the same for obesity just in case\n\nstates &lt;- states %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%\n  filter(name != \"district of columbia\" & name != \"puerto rico\" ) #need to remove these two from data since obesity data does not have district of columbia and puerto rico \n\nstates$above_below &lt;- obesity$above_below # Add above below to states data\n\n# obesity %&gt;%\n#   anti_join(states, by = c(\"state\" = \"name\"))\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~obesity_pal(above_below), #use the pal from above\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = obesity_pal, values = obesity$above_below, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data"
  },
  {
    "objectID": "mini1.html",
    "href": "mini1.html",
    "title": "Mapping Project",
    "section": "",
    "text": "Static Plots\nThe first statistic I would like to investigate are the literacy rates among states. Literacy rates are incredibly important and have sweeping effects on people’s employment opportunities, educational attainment and overall quality of life. The data comes from the The World Population Review though it does not state the exact literacy rates. In actuality, it gives the percentage of the adult population that fall under the category of low literacy rates for 2024. According to the National Center for Education Statistics, low literacy is defined as “those performing at PIAAC literacy proficiency level 1 or below or who could not participate due to a language barrier or a cognitive or physical inability to be interviewed.” These adults are able to process meaning at the sentence level meaning they can only read short, simple paragraphs. (https://nces.ed.gov/surveys/piaac/measure.asp)\nIn order to accomplish this graph, we use geometic map data of the US states.\n\nus_states &lt;- map_data(\"state\") #This contains the geometric map data\nliteracy &lt;- read_csv(\"~/Desktop/data science/us.-literacy-rates-by-state-2024 (1).csv\") \n\n\nliteracy &lt;- literacy |&gt;\n  mutate(state = str_to_lower(state)) %&gt;%\n  rename(low_lit = LiteracyRatesPercofPopulationWithLowLiteracy)\n\nliteracy |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt; #state in literacy and region in us_states\n  rename(region = state) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = low_lit), color = \"black\")+\n  labs(\n    title = \"Map of the US showing % of State Population with Low Literacy\",\n    fill = \"% of Population with Low Literacy\",\n    caption = \"Source: The World Population Review\"\n  )+\n  coord_map() + #makes it scale correctly \n  theme_void() + \n  scale_fill_viridis(option = \"E\")\n\n\n\n\n\n\n\n\nThis is a color coded choropleth map showing the percentage of the state population that has a low literacy rate. The legend shows that the percentage of low literacy ranges from about 0 to over 25% with darker blue colors representing a small percentage of low literacy ranging to brighter yellow that represents a higher percentage of low literacy. We notice that southern states appear to have higher percentages of low literacy as notice they appear more yellow in color. Texas, California and New Mexico stand out as having the highest percentage. In contrast, northern states appear to have lower percentages of low literacy especially upper New England such as New Hampshire and Midwest states like Minnesota and Montana.\n\nMy second statistic I would like to to investigate are obesity rates in America. The data was sourced from Data.Gov gives the state obesity rate for 2015. Obesity is a growing problem in America, as it poses significant health risks to thousands of people. However different states have varying levels of obesity. Here we can investigate which states are above and below the average obesity percentage in America to see if there are any patterns in obesity rates.\n\nobesity &lt;- read_csv(\"~/Desktop/data science/LakeCounty_Health_-6177935595181947989.csv\")\n\nobesity &lt;- obesity %&gt;%\n  rename(state = NAME) %&gt;%\n  mutate(state = str_to_lower(state)) %&gt;%\n  mutate(above_below = ifelse(Obesity &gt; mean(Obesity), \"Above\", \"Below\")) \n\n\nobesity |&gt;\n  mutate(state = str_to_lower(state)) %&gt;%\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  rename(region = state) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = above_below), color = \"darkgrey\", linewidth = 0.4) + \n  labs(fill = \"Above or Below Mean\",\ncaption = \"Source: Data.Gov\",\ntitle = \"Graph showing states with Obesity Rates Above or Below Average Rate\") +\n  coord_map() + \n  theme_void() +\n  scale_fill_viridis(discrete = TRUE, option = \"E\") #discrete = T makes it work for categorical data\n\n\n\n\n\n\n\n  #scale_fill_manual()\n\nFrom the graph above, we notice that the majority of states with obesity rates above the average are in the middle of the United States on the right side. The Left side of the United States remains mostly below the average apart from Oregon. We also notice Minnesota as anomaly in the middle being below the average obesity rate. This may point to cultural reasons for obesity, as many adjacent states share the same category, likely having similar food customs or ways of life that may lead to obesity. The spread is also vertical with similar categories spanning the length of the US and being split in half by the breadth. This map may be further enhanced if it could be cross referenced with a map showing the prevalence of fast food chains such as McDonald’s.\n\n\n\nInteractive Plots\nBelow are more plot that provide interactive information on literacy. Here you can hover over a state and see the obesity and literary rate so that you can easily compare it with their neighboring states.\n\nlibrary(sf) \n\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")  \n\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0,11,15,20,25,29,Inf)\npal &lt;- colorBin(\"inferno\", domain = literacy$low_lit, bins = bins)\n\nliteracy &lt;- literacy |&gt;\n  mutate(labels = str_c(str_to_title(state), \": \", low_lit))\n\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n\nlabels &lt;- lapply(literacy$labels, HTML)\n\nstates &lt;- states %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%\n  filter(name != \"district of columbia\" & name != \"puerto rico\" )\n\n\nstates$low_lit &lt;- literacy$low_lit #need to add it like this so that the spacial data remains because joining causes it to lose the sf\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(low_lit),\n    weight = 2,\n    opacity = 1,\n    smoothFactor = 0.5,\n    color = \"white\",\n    dashArray = \"2\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~literacy$low_lit, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\nObesity map\n\n####################\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n###################\n\n\nobesity_pal &lt;- colorFactor(viridis(2), states$above_below) #This creates a color factor for above and below\n\nobesity &lt;- obesity |&gt;\n  mutate(labels = str_c(str_to_title(state), \": \", Obesity)) %&gt;% #show the obesity rate when you hover over\n  arrange(state) #need to arrange so when merged with states data it lines up correctly for obesity\n\nlabels &lt;- lapply(obesity$labels, HTML) #\n\nobesity &lt;- obesity %&gt;%\n   filter(state != \"district of columbia\" & state != \"puerto rico\" ) #do the same for obesity just in case\n\nstates &lt;- states %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%\n  filter(name != \"district of columbia\" & name != \"puerto rico\" ) #need to remove these two from data since obesity data does not have district of columbia and puerto rico \n\nstates$above_below &lt;- obesity$above_below # Add above below to states data\n\n# obesity %&gt;%\n#   anti_join(states, by = c(\"state\" = \"name\"))\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~obesity_pal(above_below), #use the pal from above\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = obesity_pal, values = obesity$above_below, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\nLiteracy Map"
  }
]